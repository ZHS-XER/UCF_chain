# UCF 视频理解实验

这个项目实现了两个基于大型语言模型 (LLM) 的视频理解实验，主要利用 Google 的 Gemini 模型评估其对视频内容的理解能力。实验基于 UCF 数据集中的体育动作视频，通过抽取视频帧来测试模型的时序理解能力。

## 项目概述

本项目包含两个主要实验：
1. **多选题视频理解实验**：向模型展示视频的前几帧，让模型预测后续发生的情况
2. **视频帧排序实验**：向模型提供打乱顺序的视频帧，测试模型能否正确还原时间顺序

## 数据集

实验使用 UCF 动作识别数据集，包含多种体育活动类别的视频，如：
- Basketball（篮球）
- Diving（跳水）
- GolfSwing（高尔夫挥杆）
- HighJump（跳高）
- JavelinThrow（标枪）
- LongJump（跳远）
- PoleVault（撑杆跳）
- Surfing（冲浪）

每个视频被分解成一系列帧，存储在 `data/frames/` 目录下，按类别和视频 ID 组织。

## 实验一：多选题视频理解（MCQ）

### 实现原理

1. 将视频的前几帧（通常是 2-3 帧）作为"题干"展示给模型
2. 提供四个候选帧，其中只有一个是视频的真实后续帧（正确答案）
3. 其余三个为干扰项，来自其他视频或其他时间点
4. 要求模型预测哪一个帧最有可能是视频的下一帧

### 功能特点

- 模型需要描述每一帧的内容
- 提供详细的推理过程
- 给出预测的答案（0-3 之间的选择）
- 结果以 JSON 格式保存，便于后续分析

### 使用方法

```bash
python main.py
```

结果将保存在 `results_per_question` 目录中，每个问题生成一个 JSON 文件。

## 实验二：视频帧排序实验

### 实现原理

1. 从同一个视频中随机抽取 5 帧
2. 将这些帧打乱顺序展示给模型
3. 要求模型预测这些帧的正确时间顺序
4. 测试模型对动作连续性的理解

### 功能特点

- 为防止模型从文件名推断顺序，使用随机标识符代替真实文件名
- 模型需要详细描述每一帧的内容
- 提供详细的排序推理过程
- 给出预测的帧顺序（0-4 之间的索引排列）
- 结果以 JSON 格式保存，包含原始顺序和预测顺序

### 使用方法

```bash
python main_ordering.py
```

结果将保存在 `results_ordering` 目录中，每个视频生成一个 JSON 文件。

## 结果分析

项目包含结果分析工具，可生成总结报告：

```bash
python generate_summary.py
```

生成的总结报告 `ordering_summary.json` 包含：
- 每个视频的预测顺序和正确顺序
- 整体正确率统计
- 按动作类别的正确率统计

## 技术实现

- 使用 Google Gemini 2.5 Flash 视觉语言模型
- 异步处理多个视频测试任务
- 结构化 JSON 输出便于结果分析
- 支持断点续传，可以从中断处继续实验

## 项目结构

- `main.py`: 多选题实验主程序
- `main_ordering.py`: 排序实验主程序
- `build_mcq.py`: 构建多选题数据集
- `build_ordering.py`: 构建排序任务数据集
- `extract.py`: 从视频中提取帧
- `generate_summary.py`: 生成结果总结报告
- `data/`: 数据目录
  - `frames/`: 视频帧图像
  - `ordering.json`: 排序任务定义
- `results_ordering/`: 排序实验结果
- `results_per_question/`: 多选题实验结果

## 主要发现

通过这些实验，我们可以评估视觉语言模型在以下能力方面的表现：
1. 理解动作的连续性和时序关系
2. 基于前几帧预测视频后续发展
3. 对不同类型体育活动的理解程度
4. 在有限视觉信息下进行推理的能力

这些测试有助于评估大型视觉语言模型在视频理解任务中的能力边界，对未来更复杂的视频理解和预测任务提供基础。
